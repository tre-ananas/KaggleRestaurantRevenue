) %>%
# Remove old date column
step_rm(`Open Date`) %>%
# Break down date into more specific features
step_date(Open_Date, features = c('month', 'quarter')) %>%
# Extract Season
step_mutate(season = factor(case_when(
between(month(Open_Date), 3, 5) ~ "Spring",
between(month(Open_Date), 6, 8) ~ "Summer",
between(month(Open_Date), 9, 11) ~ "Fall",
TRUE ~ "Winter"
)))  %>%
# Remove Open_Date now that we have better features
step_rm(Open_Date) %>%
# Remove city names because there are new cities in the test data
step_rm(City) %>%
# Integers to factors
step_mutate_at(all_integer_predictors(), fn = factor) %>%
# Ignoring P-variables, turn appropriate variables into factors
step_mutate_at(c('City Group'), fn = factor) %>%
# Dummy factors
step_dummy(all_nominal_predictors()) %>%
# Median imputation for missing quantitative values
step_impute_median(population) %>%
step_impute_median(`per capita annual income`) %>%
step_impute_median(`percentage w higher ed or more`) %>%
# Normalize numeric features
step_normalize(all_numeric_predictors()) %>%
# Remove zero-variance variables
step_zv()%>%
# Remove features correlated above .85
step_corr(all_numeric_predictors(), threshold = 0.85) %>%
# PCR Threshold = .85
step_pca(all_numeric_predictors(), threshold = .85)
# Prep, Bake, and View Recipe
prepped <- prep(rec)
baked <- bake(prepped, full_train) %>%
head(baked, 5)
# Create Recipe
rec <- recipe(revenue ~ ., data = full_train) %>%
# Remove ID; Remove Type because EDA showed some problems
step_rm(Id, Type) %>%
# Create a date column with proper format
step_mutate(
Open_Date = as.Date(`Open Date`, format = "%m/%d/%Y")
) %>%
# Remove old date column
step_rm(`Open Date`) %>%
# Break down date into more specific features
step_date(Open_Date, features = c('month', 'quarter')) %>%
# Extract Season
step_mutate(season = factor(case_when(
between(month(Open_Date), 3, 5) ~ "Spring",
between(month(Open_Date), 6, 8) ~ "Summer",
between(month(Open_Date), 9, 11) ~ "Fall",
TRUE ~ "Winter"
)))  %>%
# Remove Open_Date now that we have better features
step_rm(Open_Date) %>%
# Remove city names because there are new cities in the test data
step_rm(City) %>%
# Integers to factors
step_mutate_at(all_integer_predictors(), fn = factor) %>%
# Ignoring P-variables, turn appropriate variables into factors
step_mutate_at(c('City Group'), fn = factor) %>%
# Dummy factors
step_dummy(all_nominal_predictors()) %>%
# Median imputation for missing quantitative values
step_impute_median(population) %>%
step_impute_median(`per capita annual income`) %>%
step_impute_median(`percentage w higher ed or more`) %>%
# Normalize numeric features
step_normalize(all_numeric_predictors()) %>%
# Remove zero-variance variables
step_zv()%>%
# Remove features correlated above .85
step_corr(all_numeric_predictors(), threshold = 0.85) %>%
# PCR Threshold = .85
step_pca(all_numeric_predictors(), threshold = .85)
# Prep, Bake, and View Recipe
prepped <- prep(rec)
baked <- bake(prepped, full_train) %>%
head(baked)
##### Recipe
# Create Recipe
rec <- recipe(revenue ~ ., data = full_train) %>%
# Remove ID; Remove Type because EDA showed some problems
step_rm(Id, Type) %>%
# Create a date column with proper format
step_mutate(
Open_Date = as.Date(`Open Date`, format = "%m/%d/%Y")
) %>%
# Remove old date column
step_rm(`Open Date`) %>%
# Break down date into more specific features
step_date(Open_Date, features = c('month', 'quarter')) %>%
# Extract Season
step_mutate(season = factor(case_when(
between(month(Open_Date), 3, 5) ~ "Spring",
between(month(Open_Date), 6, 8) ~ "Summer",
between(month(Open_Date), 9, 11) ~ "Fall",
TRUE ~ "Winter"
)))  %>%
# Remove Open_Date now that we have better features
step_rm(Open_Date) %>%
# Remove city names because there are new cities in the test data
step_rm(City) %>%
# Integers to factors
step_mutate_at(all_integer_predictors(), fn = factor) %>%
# Ignoring P-variables, turn appropriate variables into factors
step_mutate_at(c('City Group'), fn = factor) %>%
# Dummy factors
step_dummy(all_nominal_predictors()) %>%
# Median imputation for missing quantitative values
step_impute_median(population) %>%
step_impute_median(`per capita annual income`) %>%
step_impute_median(`percentage w higher ed or more`) %>%
# Normalize numeric features
step_normalize(all_numeric_predictors()) %>%
# Remove zero-variance variables
step_zv()%>%
# Remove features correlated above .85
step_corr(all_numeric_predictors(), threshold = 0.85) %>%
# PCR Threshold = .85
step_pca(all_numeric_predictors(), threshold = .85)
# Prep, Bake, and View Recipe
prepped <- prep(rec)
baked <- bake(prepped, full_train)
head(baked, 3)
##### Modeling
# Set up model
random_forest <- rand_forest(mtry = tune(),
min_n = tune(),
trees = 1000) %>% # Type of Model
set_engine("ranger") %>% # What R function to use
set_mode("regression")
# Workflow
wf <- workflow() %>%
add_recipe(rec) %>%
add_model(random_forest) %>%
fit(data = full_train) # Fit the workflow
# Grid of values to tune over
tg <- grid_regular(mtry(range = c(1, 13)),
min_n(),
levels = 5)
# Split data for cross-validation (CV)
folds <- vfold_cv(full_train, v = 5, repeats = 1)
# Run cross-validation
cv_results <- wf %>%
tune_grid(resamples = folds,
grid = tg,
metrics = metric_set(rmse))
# Find best tuning parameters
best_tune <- cv_results %>%
select_best("rmse")
# Finalize workflow and fit it
final_wf <- wf %>%
finalize_workflow(best_tune) %>%
fit(data = full_train)
best_tune
# Predict Sales for Each ID in full_test
predictions <- bind_cols(full_test$Id,
predict(final_wf, new_data = full_test)) %>% # Bind predictions to corresponding IDs
rename("Id" = "...1", "Prediction" = ".pred")# Rename columns
# Write Predictions to .csv
vroom_write(x=predictions, file="random_forest_1_second_try_predictions.csv", delim = ",")
########## Load Data and Packages
# Packages
library(vroom) # Loading Data
library(tidymodels) # Modeling and cross validation
library(tidyverse) # Everything, really
library(DataExplorer) # EDA
library(patchwork) # Plots
library(GGally) # EDA
library(naivebayes) # Naive Bayes
library(discrim) # PCR
library(lubridate) # Dates
library(embed) # Extra recipe steps
library(workflows) # Workflows
library(bonsai)
library(lightgbm)
# Data
train <- vroom('train.csv')
test <- vroom('test.csv')
turkey <- vroom('turkey.csv')
########## Clean and Merge
# Assuming your data frame is named 'turkey'
turkey <- turkey %>%
# Remove redundant columns
select(-latitude, -longitude, -`plate code`) %>%
# Change column name 'city' to 'City'
rename(City = city) %>%
# Remove dots in 'population' column and convert to numeric
mutate(population = as.numeric(gsub("\\.", "", population))) %>%
# Format 'per capita annual income' column
rename(`per capita annual income` = "
per capita annual income") %>%
mutate(`per capita annual income` = as.numeric(gsub(" TL", "", gsub("\\.", "", `per capita annual income`)))) %>%
# Format 'number of people with higher education and above' column
rename(`percentage w higher ed or more` = "number of people with higher education and above") %>%
mutate(`percentage w higher ed or more` = as.numeric(gsub("%", "", `percentage w higher ed or more`)))
# Merge turkey dataset with training and testing data
full_train <- merge(train, turkey, by = "City", all.x = TRUE)
full_test <- merge(test, turkey, by = "City", all.x = TRUE)
View(train)
rec <- recipe(revenue ~ ., data = full_train) %>%
# Remove ID; Remove Type because EDA showed some problems
step_rm(Id, Type) %>%
# Create a date column with proper format
step_mutate(
Open_Date = as.Date(`Open Date`, format = "%m/%d/%Y")
) %>%
# Remove old date column
step_rm(`Open Date`) %>%
# Break down date into more specific features
step_date(Open_Date, features = c('month', 'quarter')) %>%
# Extract Season
step_mutate(season = factor(case_when(
between(month(Open_Date), 3, 5) ~ "Spring",
between(month(Open_Date), 6, 8) ~ "Summer",
between(month(Open_Date), 9, 11) ~ "Fall",
TRUE ~ "Winter"
)))  %>%
# Remove Open_Date now that we have better features
step_rm(Open_Date) %>%
# Remove city names because there are new cities in the test data
step_rm(City) %>%
# Integers to factors
step_mutate_at(all_integer_predictors(), fn = factor) %>%
# Ignoring P-variables, turn appropriate variables into factors
step_mutate_at(c('City Group'), fn = factor) %>%
# Dummy factors
step_dummy(all_nominal_predictors()) %>%
# Median imputation for missing quantitative values
step_impute_median(population) %>%
step_impute_median(`per capita annual income`) %>%
step_impute_median(`percentage w higher ed or more`) %>%
# Normalize numeric features
step_normalize(all_numeric_predictors()) %>%
# Remove zero-variance variables
step_zv()%>%
# Remove features correlated more than .85
step_corr(all_numeric_predictors(), threshold = 0.85) %>%
# PCR Threshold = .85
step_pca(all_numeric_predictors(), threshold = .85)
# Prep, Bake, and View Recipe
prepped <- prep(rec)
baked <- bake(prepped, full_train) %>%
slice(1:10)
##### Recipe
# Create Recipe
rec <- recipe(revenue ~ ., data = full_train) %>%
# Remove ID; Remove Type because EDA showed some problems
step_rm(Id, Type) %>%
# Create a date column with proper format
step_mutate(
Open_Date = as.Date(`Open Date`, format = "%m/%d/%Y")
) %>%
# Remove old date column
step_rm(`Open Date`) %>%
# Break down date into more specific features
step_date(Open_Date, features = c('month', 'quarter')) %>%
# Extract Season
step_mutate(season = factor(case_when(
between(month(Open_Date), 3, 5) ~ "Spring",
between(month(Open_Date), 6, 8) ~ "Summer",
between(month(Open_Date), 9, 11) ~ "Fall",
TRUE ~ "Winter"
)))  %>%
# Remove Open_Date now that we have better features
step_rm(Open_Date) %>%
# Remove city names because there are new cities in the test data
step_rm(City) %>%
# Integers to factors
step_mutate_at(all_integer_predictors(), fn = factor) %>%
# Ignoring P-variables, turn appropriate variables into factors
step_mutate_at(c('City Group'), fn = factor) %>%
# Dummy factors
step_dummy(all_nominal_predictors()) %>%
# Median imputation for missing quantitative values
step_impute_median(population) %>%
step_impute_median(`per capita annual income`) %>%
step_impute_median(`percentage w higher ed or more`) %>%
# Normalize numeric features
step_normalize(all_numeric_predictors()) %>%
# Remove zero-variance variables
step_zv()%>%
# Remove features correlated above .85
step_corr(all_numeric_predictors(), threshold = 0.85) %>%
# PCR Threshold = .85
step_pca(all_numeric_predictors(), threshold = .85)
# Prep, Bake, and View Recipe
prepped <- prep(rec)
baked <- bake(prepped, full_train)
View(baked)
# Create Recipe
rec <- recipe(revenue ~ ., data = full_train) %>%
# Remove ID; Remove Type because EDA showed some problems
step_rm(Id, Type) %>%
# Create a date column with proper format
step_mutate(
Open_Date = as.Date(`Open Date`, format = "%m/%d/%Y")
) %>%
# Remove old date column
step_rm(`Open Date`) %>%
# Break down date into more specific features
step_date(Open_Date, features = c('month', 'quarter')) %>%
# Extract Season
step_mutate(season = factor(case_when(
between(month(Open_Date), 3, 5) ~ "Spring",
between(month(Open_Date), 6, 8) ~ "Summer",
between(month(Open_Date), 9, 11) ~ "Fall",
TRUE ~ "Winter"
)))
# Prep, Bake, and View Recipe
prepped <- prep(rec)
baked <- bake(prepped, full_train)
# Create Recipe
rec <- recipe(revenue ~ ., data = full_train) %>%
# Remove ID; Remove Type because EDA showed some problems
step_rm(Id, Type) %>%
# Create a date column with proper format
step_mutate(
Open_Date = as.Date(`Open Date`, format = "%m/%d/%Y")
) %>%
# Remove old date column
step_rm(`Open Date`) %>%
# Break down date into more specific features
step_date(Open_Date, features = c('month', 'quarter')) %>%
# Extract Season
step_mutate(season = factor(case_when(
between(month(Open_Date), 3, 5) ~ "Spring",
between(month(Open_Date), 6, 8) ~ "Summer",
between(month(Open_Date), 9, 11) ~ "Fall",
TRUE ~ "Winter"
))) %>%
# Remove Open_Date now that we have better features
step_rm(Open_Date) %>%
# Remove city names because there are new cities in the test data
step_rm(City) %>%
# Integers to factors
step_mutate_at(all_integer_predictors(), fn = factor) %>%
# Ignoring P-variables, turn appropriate variables into factors
step_mutate_at(c('City Group'), fn = factor) %>%
# Dummy factors
step_dummy(all_nominal_predictors()) %>%
# Median imputation for missing quantitative values
step_impute_median(population) %>%
step_impute_median(`per capita annual income`) %>%
step_impute_median(`percentage w higher ed or more`) %>%
# Normalize numeric features
step_normalize(all_numeric_predictors()) %>%
# Remove zero-variance variables
step_zv()%>%
# Remove features correlated above .85
step_corr(all_numeric_predictors(), threshold = 0.85) %>%
# PCR Threshold = .85
step_pca(all_numeric_predictors(), threshold = .85)
# Create Recipe
rec <- recipe(revenue ~ ., data = full_train) %>%
# Remove ID; Remove Type because EDA showed some problems; remove city names because there are new cities in the test data;
# remove date because that seems not as important as the other features all things considered
step_rm(Id, Type, `Open Date`, City) %>%
# Remove city names because there are new cities in the test data
step_rm(City) %>%
# Integers to factors
step_mutate_at(all_integer_predictors(), fn = factor) %>%
# Ignoring P-variables, turn appropriate variables into factors
step_mutate_at(c('City Group'), fn = factor) %>%
# Dummy factors
step_dummy(all_nominal_predictors()) %>%
# Median imputation for missing quantitative values
step_impute_median(population) %>%
step_impute_median(`per capita annual income`) %>%
step_impute_median(`percentage w higher ed or more`) %>%
# Normalize numeric features
step_normalize(all_numeric_predictors()) %>%
# Remove zero-variance variables
step_zv()%>%
# Remove features correlated above .85
step_corr(all_numeric_predictors(), threshold = 0.85) %>%
# PCR Threshold = .85
step_pca(all_numeric_predictors(), threshold = .85)
# Prep, Bake, and View Recipe
prepped <- prep(rec)
baked <- bake(prepped, full_train)
# Create Recipe
rec <- recipe(revenue ~ ., data = full_train) %>%
# Remove ID; Remove Type because EDA showed some problems; remove city names because there are new cities in the test data;
# remove date because that seems not as important as the other features all things considered
step_rm(Id, Type, `Open Date`, City) %>%
# Integers to factors
step_mutate_at(all_integer_predictors(), fn = factor) %>%
# Ignoring P-variables, turn appropriate variables into factors
step_mutate_at(c('City Group'), fn = factor) %>%
# Dummy factors
step_dummy(all_nominal_predictors()) %>%
# Median imputation for missing quantitative values
step_impute_median(population) %>%
step_impute_median(`per capita annual income`) %>%
step_impute_median(`percentage w higher ed or more`) %>%
# Normalize numeric features
step_normalize(all_numeric_predictors()) %>%
# Remove zero-variance variables
step_zv()%>%
# Remove features correlated above .85
step_corr(all_numeric_predictors(), threshold = 0.85) %>%
# PCR Threshold = .85
step_pca(all_numeric_predictors(), threshold = .85)
# Prep, Bake, and View Recipe
prepped <- prep(rec)
baked <- bake(prepped, full_train)
head(baked, 3)
##### Modeling
# Set up model
random_forest <- rand_forest(mtry = tune(),
min_n = tune(),
trees = 1000) %>% # Type of Model
set_engine("ranger") %>% # What R function to use
set_mode("regression")
# Workflow
wf <- workflow() %>%
add_recipe(rec) %>%
add_model(random_forest) %>%
fit(data = full_train) # Fit the workflow
# Grid of values to tune over
tg <- grid_regular(mtry(range = c(1, 13)),
min_n(),
levels = 5)
# Split data for cross-validation (CV)
folds <- vfold_cv(full_train, v = 5, repeats = 1)
# Run cross-validation
cv_results <- wf %>%
tune_grid(resamples = folds,
grid = tg,
metrics = metric_set(rmse))
# Find best tuning parameters
best_tune <- cv_results %>%
select_best("rmse")
# Finalize workflow and fit it
final_wf <- wf %>%
finalize_workflow(best_tune) %>%
fit(data = full_train)
##### Predictions
# Predict Sales for Each ID in full_test
predictions <- bind_cols(full_test$Id,
predict(final_wf, new_data = full_test)) %>% # Bind predictions to corresponding IDs
rename("Id" = "...1", "Prediction" = ".pred")# Rename columns
final_wf
best_tune
# Write Predictions to .csv
vroom_write(x=predictions, file="random_forest_6_predictions.csv", delim = ",")
# Packages
library(vroom) # Loading Data
library(tidymodels) # Modeling and cross validation
library(tidyverse) # Everything, really
library(DataExplorer) # EDA
library(patchwork) # Plots
library(GGally) # EDA
library(naivebayes) # Naive Bayes
library(discrim) # PCR
library(lubridate) # Dates
library(embed) # Extra recipe steps
library(workflows) # Workflows
library(bonsai)
library(lightgbm)
# Data
train <- vroom('train.csv')
test <- vroom('test.csv')
turkey <- vroom('turkey.csv')
View(train)
View(test)
rge
########## Load Data and Packages
# Packages
library(vroom) # Loading Data
library(tidymodels) # Modeling and cross validation
library(tidyverse) # Everything, really
library(DataExplorer) # EDA
library(patchwork) # Plots
library(GGally) # EDA
library(naivebayes) # Naive Bayes
library(discrim) # PCR
library(lubridate) # Dates
library(embed) # Extra recipe steps
library(workflows) # Workflows
library(bonsai)
library(lightgbm)
# Data
train <- vroom('train.csv')
test <- vroom('test.csv')
turkey <- vroom('turkey.csv')
########## Clean and Merge
# Assuming your data frame is named 'turkey'
turkey <- turkey %>%
# Remove redundant columns
select(-latitude, -longitude, -`plate code`) %>%
# Change column name 'city' to 'City'
rename(City = city) %>%
# Remove dots in 'population' column and convert to numeric
mutate(population = as.numeric(gsub("\\.", "", population))) %>%
# Format 'per capita annual income' column
rename(`per capita annual income` = "
per capita annual income") %>%
mutate(`per capita annual income` = as.numeric(gsub(" TL", "", gsub("\\.", "", `per capita annual income`)))) %>%
# Format 'number of people with higher education and above' column
rename(`percentage w higher ed or more` = "number of people with higher education and above") %>%
mutate(`percentage w higher ed or more` = as.numeric(gsub("%", "", `percentage w higher ed or more`)))
# Merge turkey dataset with training and testing data
full_train <- merge(train, turkey, by = "City", all.x = TRUE)
full_test <- merge(test, turkey, by = "City", all.x = TRUE)
View(full_train)
